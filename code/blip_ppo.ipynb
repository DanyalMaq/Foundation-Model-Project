{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune BLIP model to Generate Negative Responses\n",
    "\n",
    "Fine-tune a BLIP model to produce negative responses. A BERT sentiment classifier is used as a reward function. The BLIP model is trained with PPO using the classifier's reward signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Sadman/venv/lib/python3.10/site-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"Salesforce/blip-vqa-base\",\n",
    "    learning_rate=1e-4,\n",
    "    log_with=\"wandb\",\n",
    "    steps=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ml5cd6pn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-wave-10</strong> at: <a href='https://wandb.ai/sadman-ai/AI-Dev/runs/ml5cd6pn' target=\"_blank\">https://wandb.ai/sadman-ai/AI-Dev/runs/ml5cd6pn</a><br/> View project at: <a href='https://wandb.ai/sadman-ai/AI-Dev' target=\"_blank\">https://wandb.ai/sadman-ai/AI-Dev</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241024_105058-ml5cd6pn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ml5cd6pn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/Sadman/ai-dev/foundation/trl/wandb/run-20241024_112737-8s5s723z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sadman-ai/AI-Dev/runs/8s5s723z' target=\"_blank\">astral-gorge-11</a></strong> to <a href='https://wandb.ai/sadman-ai/AI-Dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sadman-ai/AI-Dev' target=\"_blank\">https://wandb.ai/sadman-ai/AI-Dev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sadman-ai/AI-Dev/runs/8s5s723z' target=\"_blank\">https://wandb.ai/sadman-ai/AI-Dev/runs/8s5s723z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sadman-ai/AI-Dev/runs/8s5s723z?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f39d3d670d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb \n",
    "\n",
    "wandb.init(project=\"ai-dev\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Models\n",
    "### Load IMDB Dataset\n",
    "Now, we will build the dataset for training. The dataset consists of the starting few words from IMDB reviews. The IMDB dataset contains 50k movie review annotated with \"positive\"/\"negative\" feedback. First, we filter out comments that are longer than 200 characters and take starting text with token size between 2 to 8.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config):\n",
    "    dataset_name = \"stanfordnlp/imdb\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    ds = load_dataset(dataset_name)\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "    \n",
    "    input_size = LengthSampler(2, 8)\n",
    "    \n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[:input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "    \n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 127/49776 [00:00<01:20, 620.14 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 49776/49776 [01:30<00:00, 549.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(config)\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained GPT2 models\n",
    "\n",
    "Load GPT2 twice. First is optimized, second is reference model for KL-divergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT Classifier \n",
    "We load a BERT classifier fine-tuned on IMDB dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device\n",
    ")\n",
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 2.5600204467773438},\n",
       " {'label': 'POSITIVE', 'score': -2.9452600479125977}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I found the acting, direction and story of this movie terrible. There was a boring vibe all along.\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 2.2316293716430664},\n",
       " {'label': 'NEGATIVE', 'score': -1.9940400123596191}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"It was an astonishing one. The plot had excitement and unpredictability. The graphics were carefully crafted.\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_scores(queries, responses):\n",
    "    texts = [q + r for q, r in zip(queries, responses)]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    neg_scores = [item[\"score\"] for output in pipe_outputs for item in output if item[\"label\"] == \"NEGATIVE\"]\n",
    "    return neg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Sadman/venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n",
      "/users/Sadman/venv/lib/python3.10/site-packages/accelerate/accelerator.py:443: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "subset_length = 3000\n",
    "indices = list(range(subset_length))\n",
    "subset = Subset(dataset[\"train\"], indices) # dataset has \"train\", \"test\", and \"unsupervised\"\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config, model, ref_model, tokenizer, dataset=subset, data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "Training loop consists of three main steps:\n",
    "1. Get query responses from policy network (GPT-2)\n",
    "2. Get sentiments for query/responses from BERT \n",
    "3. Optimize policy with PPO using the (query, response, reward) triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [10:53<00:00, 28.42s/it]\n"
     ]
    }
   ],
   "source": [
    "output_min_len = 10\n",
    "output_max_len = 20\n",
    "\n",
    "output_length_sampler = LengthSampler(output_min_len, output_max_len)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    \n",
    "    # Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs).squeeze()\n",
    "        response_tensors.append(response[len(query):])\n",
    "    batch[\"response\"] = [tokenizer.decode(response) for response in response_tensors]\n",
    "    \n",
    "    # Compute sentiment score\n",
    "    scores = get_sent_scores(batch[\"query\"], batch[\"response\"])\n",
    "    rewards = [torch.tensor(score) for score in scores]\n",
    "    \n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_797656/3722407762.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query = torch.tensor(query_tensors[i]).to(device)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The word 'classic'</td>\n",
       "      <td>doesn't really suffice, since this film is fa...</td>\n",
       "      <td>- deperson and things to elevate the clichés ...</td>\n",
       "      <td>1.802846</td>\n",
       "      <td>-0.073480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What an inspiring movie, I laughed</td>\n",
       "      <td>out loud.&lt;|endoftext|&gt;</td>\n",
       "      <td>out loud a few times . I shouldn't have; I di...</td>\n",
       "      <td>-1.827979</td>\n",
       "      <td>-1.276709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For people interested in business</td>\n",
       "      <td>or WWE expertise, visit www.wwea.com (if not ...</td>\n",
       "      <td>careers such as Donald Trump, many of the exa...</td>\n",
       "      <td>-0.505880</td>\n",
       "      <td>-1.060472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eytan</td>\n",
       "      <td>\" and a fellow balloted he is son of Jimi Lupi...</td>\n",
       "      <td>the Cruel in prison (which did not even deser...</td>\n",
       "      <td>-0.501725</td>\n",
       "      <td>1.110715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This may actually be</td>\n",
       "      <td>true\" it had briefly touched an emotional cho...</td>\n",
       "      <td>a bit lame this time, especially those inevit...</td>\n",
       "      <td>-1.307753</td>\n",
       "      <td>1.274283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>this film takes you inside</td>\n",
       "      <td>a forest, services massravaged romance betwee...</td>\n",
       "      <td>the pansies of 45 - 50 bumish people, giving ...</td>\n",
       "      <td>-0.803070</td>\n",
       "      <td>-0.263245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Guys and Dolls is</td>\n",
       "      <td>a dual 'brilliant' film of the people that will</td>\n",
       "      <td>a let down. Colon is a garbage and inconceiva...</td>\n",
       "      <td>-2.425464</td>\n",
       "      <td>2.445132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Julian Noble (Pierce Bros</td>\n",
       "      <td>nan) Wickford Lane (Charles Durning</td>\n",
       "      <td>nan in \"Donnie Capote\") talks as</td>\n",
       "      <td>0.060136</td>\n",
       "      <td>-0.824049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You want</td>\n",
       "      <td>to fight back on your true respect for the so...</td>\n",
       "      <td>? Do not bother. If you love the movie</td>\n",
       "      <td>-0.953021</td>\n",
       "      <td>-0.390523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>You probably all already know this</td>\n",
       "      <td>, but I wasted my hard earned sleep for it.&lt;br...</td>\n",
       "      <td>from movies like a made or planned or repeate...</td>\n",
       "      <td>2.054764</td>\n",
       "      <td>2.273826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The script for \"Scary Movie</td>\n",
       "      <td>\" is well written, incredibly touching overall...</td>\n",
       "      <td>\" was so lame that I literally couldn't watch ...</td>\n",
       "      <td>-2.411032</td>\n",
       "      <td>2.508253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'Where the Sid</td>\n",
       "      <td>eways Go Ends Charlie\" and \" Sentinel...\"-- a</td>\n",
       "      <td>funny - this is hilarious, depressing, boring...</td>\n",
       "      <td>-0.182193</td>\n",
       "      <td>1.468123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>You can call</td>\n",
       "      <td>it first-rate slasher because it's a bloody g...</td>\n",
       "      <td>this gory. It is pathetic in a piece of trash.&lt;</td>\n",
       "      <td>0.378756</td>\n",
       "      <td>2.507959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pickup On South Street is</td>\n",
       "      <td>just a taste of the futuristic drivel gangs.I...</td>\n",
       "      <td>basically homeless people. Depressing.&lt;br /&gt;&lt;...</td>\n",
       "      <td>-2.253717</td>\n",
       "      <td>1.937862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Another double</td>\n",
       "      <td>agent who had been fascinated in Josie's past...</td>\n",
       "      <td>decoy went on with the closed cars. To quote ...</td>\n",
       "      <td>-0.139180</td>\n",
       "      <td>1.259013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>for those of you who</td>\n",
       "      <td>films rather seldom) has the lighting convert...</td>\n",
       "      <td>might be looking up Bumbling from the air. Th...</td>\n",
       "      <td>-0.554571</td>\n",
       "      <td>-0.499347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 query  \\\n",
       "0                   The word 'classic'   \n",
       "1   What an inspiring movie, I laughed   \n",
       "2    For people interested in business   \n",
       "3                                Eytan   \n",
       "4                 This may actually be   \n",
       "5           this film takes you inside   \n",
       "6                    Guys and Dolls is   \n",
       "7            Julian Noble (Pierce Bros   \n",
       "8                             You want   \n",
       "9   You probably all already know this   \n",
       "10         The script for \"Scary Movie   \n",
       "11                      'Where the Sid   \n",
       "12                        You can call   \n",
       "13           Pickup On South Street is   \n",
       "14                      Another double   \n",
       "15                for those of you who   \n",
       "\n",
       "                                    response (before)  \\\n",
       "0    doesn't really suffice, since this film is fa...   \n",
       "1                              out loud.<|endoftext|>   \n",
       "2    or WWE expertise, visit www.wwea.com (if not ...   \n",
       "3   \" and a fellow balloted he is son of Jimi Lupi...   \n",
       "4    true\" it had briefly touched an emotional cho...   \n",
       "5    a forest, services massravaged romance betwee...   \n",
       "6     a dual 'brilliant' film of the people that will   \n",
       "7                 nan) Wickford Lane (Charles Durning   \n",
       "8    to fight back on your true respect for the so...   \n",
       "9   , but I wasted my hard earned sleep for it.<br...   \n",
       "10  \" is well written, incredibly touching overall...   \n",
       "11      eways Go Ends Charlie\" and \" Sentinel...\"-- a   \n",
       "12   it first-rate slasher because it's a bloody g...   \n",
       "13   just a taste of the futuristic drivel gangs.I...   \n",
       "14   agent who had been fascinated in Josie's past...   \n",
       "15   films rather seldom) has the lighting convert...   \n",
       "\n",
       "                                     response (after)  rewards (before)  \\\n",
       "0    - deperson and things to elevate the clichés ...          1.802846   \n",
       "1    out loud a few times . I shouldn't have; I di...         -1.827979   \n",
       "2    careers such as Donald Trump, many of the exa...         -0.505880   \n",
       "3    the Cruel in prison (which did not even deser...         -0.501725   \n",
       "4    a bit lame this time, especially those inevit...         -1.307753   \n",
       "5    the pansies of 45 - 50 bumish people, giving ...         -0.803070   \n",
       "6    a let down. Colon is a garbage and inconceiva...         -2.425464   \n",
       "7                    nan in \"Donnie Capote\") talks as          0.060136   \n",
       "8              ? Do not bother. If you love the movie         -0.953021   \n",
       "9    from movies like a made or planned or repeate...          2.054764   \n",
       "10  \" was so lame that I literally couldn't watch ...         -2.411032   \n",
       "11   funny - this is hilarious, depressing, boring...         -0.182193   \n",
       "12    this gory. It is pathetic in a piece of trash.<          0.378756   \n",
       "13   basically homeless people. Depressing.<br /><...         -2.253717   \n",
       "14   decoy went on with the closed cars. To quote ...         -0.139180   \n",
       "15   might be looking up Bumbling from the air. Th...         -0.554571   \n",
       "\n",
       "    rewards (after)  \n",
       "0         -0.073480  \n",
       "1         -1.276709  \n",
       "2         -1.060472  \n",
       "3          1.110715  \n",
       "4          1.274283  \n",
       "5         -0.263245  \n",
       "6          2.445132  \n",
       "7         -0.824049  \n",
       "8         -0.390523  \n",
       "9          2.273826  \n",
       "10         2.508253  \n",
       "11         1.468123  \n",
       "12         2.507959  \n",
       "13         1.937862  \n",
       "14         1.259013  \n",
       "15        -0.499347  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "# get a batch from the dataset\n",
    "bs = 16\n",
    "gen_reviews = dict()\n",
    "# subset.set_format(\"pandas\")\n",
    "df_batch = subset.dataset[random.sample(range(len(subset.dataset)), bs)]  # subset consists of dataset and indices\n",
    "gen_reviews[\"query\"] = df_batch[\"query\"]\n",
    "query_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "# get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    query = torch.tensor(query_tensors[i]).to(device)\n",
    "\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "    query_response = ref_model.generate(\n",
    "        query.unsqueeze(0), **generation_kwargs\n",
    "    ).squeeze()\n",
    "    response_len = len(query_response) - len(query)\n",
    "    response_tensors_ref.append(query_response[-response_len:])\n",
    "\n",
    "    query_response = model.generate(\n",
    "        query.unsqueeze(0), **generation_kwargs\n",
    "    ).squeeze()\n",
    "    response_tensors.append(query_response[len(query):])\n",
    "    \n",
    "# decode responses\n",
    "gen_reviews[\"response (before)\"] = [\n",
    "    tokenizer.decode(response_tensors_ref[i]) for i in range(bs)\n",
    "]\n",
    "gen_reviews[\"response (after)\"] = [\n",
    "    tokenizer.decode(response_tensors[i]) for i in range(bs)\n",
    "]\n",
    "\n",
    "# sentiment analysis of query/response pairs before and after \n",
    "scores_before = get_sent_scores(gen_reviews[\"query\"], gen_reviews[\"response (before)\"])\n",
    "scores_after = get_sent_scores(gen_reviews[\"query\"], gen_reviews[\"response (after)\"])\n",
    "gen_reviews[\"rewards (before)\"] = scores_before\n",
    "gen_reviews[\"rewards (after)\"] = scores_after\n",
    "\n",
    "df_results = pd.DataFrame(gen_reviews)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that negative score (reward) increased for most generated reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
